{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bf0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "class AddBias(nn.Module):\n",
    "    def __init__(self, bias):\n",
    "        super(AddBias, self).__init__()\n",
    "        self._bias = nn.Parameter(bias.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "#         if x.dim() == 2:\n",
    "#             bias = self._bias.t().view(1, -1)\n",
    "#         else:\n",
    "#             bias = self._bias.t().view(1, -1, 1, 1)\n",
    "        bias = self._bias.t().view(1, -1)\n",
    "        return x + bias\n",
    "\n",
    "#Categorical\n",
    "class FixedCategorical(torch.distributions.Categorical):\n",
    "    def sample(self):\n",
    "        return super().sample().unsqueeze(-1)\n",
    "\n",
    "    def log_probs(self, actions):\n",
    "        return super().log_prob(actions.squeeze(-1)).view(actions.size(0), -1).sum(-1).unsqueeze(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        p = self.probs.masked_fill(self.probs <= 0, 1)\n",
    "        return -1 * p.mul(p.log()).sum(-1)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.probs.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "class Categorical(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Categorical, self).__init__()\n",
    "\n",
    "        init_ = lambda m: init(\n",
    "            m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), gain=0.01\n",
    "        )\n",
    "\n",
    "        self.linear = init_(nn.Linear(num_inputs, num_outputs))\n",
    "\n",
    "    def forward(self, x, mask=None,temperature=1):\n",
    "        x = F.softmax(self.linear(x)/temperature,dim=-1)\n",
    "        if mask is not None:\n",
    "            return FixedCategorical(logits=x + torch.log(mask))\n",
    "        else:\n",
    "            return FixedCategorical(logits=x)\n",
    "\n",
    "#Normal\n",
    "class FixedNormal(torch.distributions.Normal):\n",
    "    def log_probs(self, actions):\n",
    "        return super().log_prob(actions).sum(-1, keepdim=True)\n",
    "\n",
    "    def entropy(self):\n",
    "        return super().entropy().sum(-1)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "class DiagGaussian(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(DiagGaussian, self).__init__()\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0))\n",
    "\n",
    "        self.fc_mean = init_(nn.Linear(num_inputs, num_outputs))\n",
    "#         desired_init_log_std = -0.693471 #exp(..) ~= 0.5\n",
    "#         desired_init_log_std = -1.609437 #exp(..) ~=0.2\n",
    "        desired_init_log_std = -2.302585 #exp(..) ~=0.1\n",
    "        \n",
    "        self.logstd = AddBias(desired_init_log_std * torch.ones(num_outputs)) #so no state-dependent sigma\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        action_mean = self.fc_mean(x)\n",
    "#         print('action_mean',action_mean.shape,x.shape)\n",
    "        zeros = torch.zeros(action_mean.size())\n",
    "        if x.is_cuda:\n",
    "            zeros = zeros.cuda()\n",
    "\n",
    "        action_logstd = self.logstd(zeros)\n",
    "        return FixedNormal(action_mean, action_logstd.exp())\n",
    "\n",
    "class ActionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, type=\"categorical\"):\n",
    "        super(ActionHead, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.type = type\n",
    "        if type == \"categorical\":\n",
    "            self.distribution = Categorical(num_inputs=input_dim, num_outputs=output_dim)\n",
    "        elif type == \"normal\":\n",
    "            self.distribution = DiagGaussian(num_inputs=input_dim, num_outputs=output_dim)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        if self.type == \"normal\":\n",
    "            return self.distribution(input)\n",
    "        else:\n",
    "            return self.distribution(input, mask)\n",
    "\n",
    "class Pi_net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Pi_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, int(hidden_size/2))\n",
    "        self.action_heads = nn.ModuleList()\n",
    "        self.action_heads.append(ActionHead(int(hidden_size/2), 2, type='categorical'))\n",
    "        self.action_heads.append(ActionHead(int(hidden_size/2)+1, 1, type='normal'))\n",
    "        \n",
    "    def forward(self, s, deterministic=False):\n",
    "        x = F.relu(self.linear1(s))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        action_outputs=[]\n",
    "        head_outputs=[]\n",
    "        head_outputs.append(x)\n",
    "        action_type_dist = self.action_heads[0](x,mask=None)\n",
    "        if deterministic:\n",
    "            action_type = action_type_dist.mode()\n",
    "        else:\n",
    "            action_type = action_type_dist.sample()\n",
    "            \n",
    "        head_outputs.append(action_type)\n",
    "        action_outputs.append(action_type)\n",
    "        head_output = torch.cat(head_outputs, dim=-1)\n",
    "\n",
    "        head_dist = self.action_heads[1](head_output,mask=None)\n",
    "        \n",
    "        if deterministic:\n",
    "            head_action = head_dist.mode()\n",
    "        else:\n",
    "            head_action = head_dist.rsample()\n",
    "        \n",
    "        action_outputs.append(head_action)\n",
    "\n",
    "        joint_action_log_prob = action_type_dist.log_probs(action_type)\n",
    "        entropy = action_type_dist.entropy().mean()\n",
    "        \n",
    "        joint_action_log_prob += head_dist.log_probs(head_action)\n",
    "\n",
    "        entropy += head_dist.entropy().mean()\n",
    "        action_outputs = torch.cat(action_outputs,dim=-1)\n",
    "        return action_outputs, joint_action_log_prob, entropy\n",
    "    \n",
    "class Binary_deter_net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Binary_deter_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, int(hidden_size/2))\n",
    "        self.linear3 = nn.Linear(int(hidden_size/2),2)\n",
    "        \n",
    "    def forward(self, s):\n",
    "        x = F.relu(self.linear1(s))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.softmax(self.linear3(x),dim=-1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Multi_deter_net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Multi_deter_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, int(hidden_size/2))\n",
    "        self.linear3 = nn.Linear(int(hidden_size/2),2)\n",
    "        self.linear4 = nn.Linear(int(hidden_size/2)+1,1)\n",
    "        \n",
    "    def forward(self, s):\n",
    "        head_outputs=[]\n",
    "        action_outputs=[]\n",
    "        \n",
    "        x = F.relu(self.linear1(s))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x_deter = self.linear3(x)\n",
    "#         print(x_deter.argmax(-1).shape,s.shape)\n",
    "        head_outputs.append(x)\n",
    "#         head_outputs.append(x_deter.argmax(-1,keepdim=True))\n",
    "        \n",
    "        x_deter_soft = F.softmax(x_deter,dim=-1)\n",
    "        head_outputs.append(x_deter_soft.argmax(-1,keepdim=True))\n",
    "        \n",
    "        head_output = torch.cat(head_outputs, dim=-1)\n",
    "#         print(head_output.shape)\n",
    "        value = self.linear4(head_output)\n",
    "#         noise = torch.randn_like(value) * 0.1  # 标准差为0.1的高斯噪声\n",
    "#         value = value + noise\n",
    "        \n",
    "        return x_deter_soft,value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d5802",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b006d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import torch, random, copy\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import spatial\n",
    "\n",
    "class WavDataset(Dataset):\n",
    "    def __init__(self, spk_list, spk2utt, embd_dict=None,embd_dim=128):\n",
    "        self.spk_list = spk_list\n",
    "        self.spk2utt = spk2utt\n",
    "        self.embd_dict = embd_dict\n",
    "        self.embd_dim = embd_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spk2utt)\n",
    "    \n",
    "    def _extract_negative_sample(self, spk2utt, target_spk):\n",
    "        # 从字典中剔除target_spk\n",
    "        nontarget_spk_list = [spk for spk in spk2utt.keys() if spk != target_spk]\n",
    "        # 随机选择一个nontarget_spk\n",
    "        nontarget_spk = random.choice(nontarget_spk_list)\n",
    "        # 从embd_dict[nontarget_spk]中随机选择一个value\n",
    "        negative_sample = random.choice(spk2utt[nontarget_spk])\n",
    "\n",
    "        return negative_sample\n",
    "    \n",
    "    def _extract_positive_sample(self, spk2utt, target_spk, enrol_utts):\n",
    "        nonenroll_utt_list = [utt for utt in spk2utt[target_spk] if utt not in enrol_utts]\n",
    "        positive_sample = random.choice(nonenroll_utt_list)\n",
    "        return positive_sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        spk=self.spk_list[idx]\n",
    "        embds=[]\n",
    "        label = random.sample([0,1],1)[0]\n",
    "        num_enrol = random.sample([i for i in range(1,200)],1)[0]\n",
    "        enrol_embd = np.zeros((1,self.embd_dim))\n",
    "        if num_enrol >=len(self.spk2utt[spk]):\n",
    "            enrol_utts = self.spk2utt[spk]\n",
    "            num_enrol=len(self.spk2utt[spk])\n",
    "            label =0\n",
    "        else:\n",
    "            enrol_utts = random.sample(self.spk2utt[spk],num_enrol)\n",
    "            \n",
    "        if num_enrol>5:\n",
    "            for utt in enrol_utts[:5]:\n",
    "                enrol_embd += self.embd_dict[utt]\n",
    "            enrol_embd = enrol_embd/5\n",
    "            for utt in enrol_utts[5:]:\n",
    "                enrol_embd = (1-0.1)*enrol_embd + 0.1*self.embd_dict[utt]\n",
    "                if random.random() < 0.5:\n",
    "                    test_utt = self._extract_negative_sample(self.spk2utt,spk)\n",
    "                    test_embd = self.embd_dict[test_utt]\n",
    "                    result = 1 - spatial.distance.cosine(enrol_embd, test_embd)\n",
    "                    if result>0.51:\n",
    "                        enrol_embd = (1-0.1)*enrol_embd + 0.1*test_embd\n",
    "        else:\n",
    "            for utt in enrol_utts:\n",
    "                enrol_embd += self.embd_dict[utt]\n",
    "            enrol_embd = enrol_embd/len(enrol_utts)\n",
    "\n",
    "        if label:\n",
    "            test_utt = self._extract_positive_sample(self.spk2utt,spk,enrol_utts)\n",
    "        else:\n",
    "            test_utt = self._extract_negative_sample(self.spk2utt,spk)\n",
    "\n",
    "        test_embd = self.embd_dict[test_utt]\n",
    "        result = 1 - spatial.distance.cosine(enrol_embd, test_embd)\n",
    "\n",
    "        embds_cat = np.concatenate((enrol_embd,test_embd),axis=1)\n",
    "        embds_cat = torch.tensor(embds_cat, dtype=torch.float).squeeze(0)\n",
    "        action_outputs=[]\n",
    "        action_outputs.append(embds_cat)\n",
    "        action_outputs = torch.cat(action_outputs,dim=-1)\n",
    "        if label:\n",
    "            return action_outputs,label,torch.tensor([0.1])\n",
    "        else:\n",
    "            return action_outputs,label,torch.tensor([0.1*result])\n",
    "    \n",
    "class WavBatchSampler(object):\n",
    "    def __init__(self, dataset, shuffle=False, batch_size=1, drop_last=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if shuffle:\n",
    "            self.sampler = RandomSampler(dataset)\n",
    "        else:\n",
    "            self.sampler = SequentialSampler(dataset)\n",
    "\n",
    "    def _renew(self):\n",
    "        return []\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch= self._renew()\n",
    "        for idx in self.sampler:\n",
    "            batch.append((idx))\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = self._renew()\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "def acc_topk(x,y,topk=1):\n",
    "    correct = 0\n",
    "    maxk = max((1,topk))\n",
    "    y_resize = y.view(-1,1)\n",
    "    _, pred = x.topk(maxk, 1, True, True)\n",
    "    correct += torch.eq(pred, y_resize).sum().float().item()\n",
    "    return correct / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076b6f5",
   "metadata": {},
   "source": [
    "## Prepare embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20089ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DRL-TU/egs/embed/time_varying_all_T_epoch21_rank0.npy\n",
      "./DRL-TU/egs/embed/time_varying_all_T_epoch21_rank1.npy\n",
      "./DRL-TU/egs/embed/time_varying_all_T_epoch21_rank2.npy\n",
      "./DRL-TU/egs/embed/vox2dev_filter_epoch21_rank0.npy\n",
      "./DRL-TU/egs/embed/vox2dev_filter_epoch21_rank1.npy\n",
      "./DRL-TU/egs/embed/vox2dev_filter_epoch21_rank2.npy\n",
      "./DRL-TU/egs/embed/vox2dev_filter_epoch21_rank3.npy\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "embds_dicts={}\n",
    "for npy_path in glob.glob('./DRL-TU/egs/embed/time_varying_all_T_epoch21_rank*.npy'):\n",
    "    print(npy_path)\n",
    "    embds_dict = np.load(npy_path,allow_pickle=True).item()\n",
    "    embds_dicts ={**embds_dicts,**embds_dict}\n",
    "    \n",
    "for npy_path in glob.glob('./DRL-TU/egs/embed/vox2dev_filter_epoch21_rank*.npy'):\n",
    "    print(npy_path)\n",
    "    embds_dict = np.load(npy_path,allow_pickle=True).item()\n",
    "    embds_dicts ={**embds_dicts,**embds_dict}\n",
    "\n",
    "utt2spk = {i.split()[0]:i.split()[1] for i in open('./data/combine_smiiptv_vox2dev/utt2spk')}\n",
    "spk2utt = {i.split()[0]:i.split()[1:] for i in open('./data/combine_smiiptv_vox2dev/spk2utt')}\n",
    "spk_list = [i.split()[0] for i in open('./data/combine_smiiptv_vox2dev/spk2utt')]\n",
    "\n",
    "utt2dur={i.split()[0]:i.split()[1] for i in open('./data/combine_smiiptv_vox2dev/dur.scp')}\n",
    "\n",
    "spk2utt_new=defaultdict(list)\n",
    "for spk in spk2utt:\n",
    "    if spk[:2]=='id':\n",
    "        spk2utt_new[spk]=spk2utt[spk]\n",
    "    else:\n",
    "        for utt in spk2utt[spk]:\n",
    "            if float(utt2dur[utt])<1:\n",
    "                continue\n",
    "            else:\n",
    "                spk2utt_new[spk].append(utt)\n",
    "\n",
    "spk2utt = spk2utt_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede570f0",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7c72c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][1/12]\t Loss 0.6975 0.6975\tTop1 Acc 64.844 64.844\tLR 0.100000\n",
      "\n",
      "Epoch [0][2/12]\t Loss 0.6915 0.6945\tTop1 Acc 65.820 65.332\tLR 0.100000\n",
      "\n",
      "Epoch [0][3/12]\t Loss 0.6842 0.6910\tTop1 Acc 66.211 65.625\tLR 0.100000\n",
      "\n",
      "Epoch [0][4/12]\t Loss 0.6838 0.6892\tTop1 Acc 64.844 65.430\tLR 0.100000\n",
      "\n",
      "Epoch [0][5/12]\t Loss 0.6834 0.6881\tTop1 Acc 63.672 65.078\tLR 0.100000\n",
      "\n",
      "Epoch [0][6/12]\t Loss 0.6633 0.6839\tTop1 Acc 69.141 65.755\tLR 0.100000\n",
      "\n",
      "Epoch [0][7/12]\t Loss 0.6547 0.6797\tTop1 Acc 68.359 66.127\tLR 0.100000\n",
      "\n",
      "Epoch [0][8/12]\t Loss 0.6481 0.6758\tTop1 Acc 67.969 66.357\tLR 0.100000\n",
      "\n",
      "Epoch [0][9/12]\t Loss 0.6594 0.6740\tTop1 Acc 64.648 66.168\tLR 0.100000\n",
      "\n",
      "Epoch [0][10/12]\t Loss 0.6505 0.6716\tTop1 Acc 66.602 66.211\tLR 0.100000\n",
      "\n",
      "Epoch [0][11/12]\t Loss 0.6463 0.6693\tTop1 Acc 66.602 66.246\tLR 0.100000\n",
      "\n",
      "Epoch [0][12/12]\t Loss 0.6301 0.6661\tTop1 Acc 68.359 66.423\tLR 0.100000\n",
      "\n",
      "Epoch [1][1/12]\t Loss 0.6313 0.6313\tTop1 Acc 68.164 68.164\tLR 0.100000\n",
      "\n",
      "Epoch [1][2/12]\t Loss 0.6348 0.6330\tTop1 Acc 67.969 68.066\tLR 0.100000\n",
      "\n",
      "Epoch [1][3/12]\t Loss 0.6371 0.6344\tTop1 Acc 67.578 67.904\tLR 0.100000\n",
      "\n",
      "Epoch [1][4/12]\t Loss 0.6322 0.6338\tTop1 Acc 67.773 67.871\tLR 0.100000\n",
      "\n",
      "Epoch [1][5/12]\t Loss 0.6325 0.6336\tTop1 Acc 67.773 67.852\tLR 0.100000\n",
      "\n",
      "Epoch [1][6/12]\t Loss 0.6414 0.6349\tTop1 Acc 66.992 67.708\tLR 0.100000\n",
      "\n",
      "Epoch [1][7/12]\t Loss 0.6175 0.6324\tTop1 Acc 69.922 68.025\tLR 0.100000\n",
      "\n",
      "Epoch [1][8/12]\t Loss 0.6502 0.6346\tTop1 Acc 65.820 67.749\tLR 0.100000\n",
      "\n",
      "Epoch [1][9/12]\t Loss 0.6392 0.6351\tTop1 Acc 66.992 67.665\tLR 0.100000\n",
      "\n",
      "Epoch [1][10/12]\t Loss 0.6176 0.6334\tTop1 Acc 69.727 67.871\tLR 0.100000\n",
      "\n",
      "Epoch [1][11/12]\t Loss 0.6581 0.6356\tTop1 Acc 65.234 67.631\tLR 0.100000\n",
      "\n",
      "Epoch [1][12/12]\t Loss 0.6331 0.6354\tTop1 Acc 67.773 67.643\tLR 0.100000\n",
      "\n",
      "Epoch [2][1/12]\t Loss 0.6374 0.6374\tTop1 Acc 67.383 67.383\tLR 0.100000\n",
      "\n",
      "Epoch [2][2/12]\t Loss 0.6640 0.6507\tTop1 Acc 64.648 66.016\tLR 0.100000\n",
      "\n",
      "Epoch [2][3/12]\t Loss 0.6448 0.6487\tTop1 Acc 66.797 66.276\tLR 0.100000\n",
      "\n",
      "Epoch [2][4/12]\t Loss 0.6410 0.6468\tTop1 Acc 66.992 66.455\tLR 0.100000\n",
      "\n",
      "Epoch [2][5/12]\t Loss 0.6399 0.6454\tTop1 Acc 67.188 66.602\tLR 0.100000\n",
      "\n",
      "Epoch [2][6/12]\t Loss 0.6319 0.6432\tTop1 Acc 68.164 66.862\tLR 0.100000\n",
      "\n",
      "Epoch [2][7/12]\t Loss 0.6671 0.6466\tTop1 Acc 64.258 66.490\tLR 0.100000\n",
      "\n",
      "Epoch [2][8/12]\t Loss 0.6023 0.6410\tTop1 Acc 71.094 67.065\tLR 0.100000\n",
      "\n",
      "Epoch [2][9/12]\t Loss 0.6552 0.6426\tTop1 Acc 65.430 66.884\tLR 0.100000\n",
      "\n",
      "Epoch [2][10/12]\t Loss 0.6613 0.6445\tTop1 Acc 64.844 66.680\tLR 0.100000\n",
      "\n",
      "Epoch [2][11/12]\t Loss 0.6597 0.6459\tTop1 Acc 65.039 66.531\tLR 0.100000\n",
      "\n",
      "Epoch [2][12/12]\t Loss 0.6499 0.6462\tTop1 Acc 66.016 66.488\tLR 0.100000\n",
      "\n",
      "Epoch [3][1/12]\t Loss 0.6329 0.6329\tTop1 Acc 67.773 67.773\tLR 0.100000\n",
      "\n",
      "Epoch [3][2/12]\t Loss 0.6644 0.6486\tTop1 Acc 64.453 66.113\tLR 0.100000\n",
      "\n",
      "Epoch [3][3/12]\t Loss 0.6410 0.6461\tTop1 Acc 66.992 66.406\tLR 0.100000\n",
      "\n",
      "Epoch [3][4/12]\t Loss 0.6550 0.6483\tTop1 Acc 65.234 66.113\tLR 0.100000\n",
      "\n",
      "Epoch [3][5/12]\t Loss 0.6428 0.6472\tTop1 Acc 66.602 66.211\tLR 0.100000\n",
      "\n",
      "Epoch [3][6/12]\t Loss 0.6289 0.6442\tTop1 Acc 68.164 66.536\tLR 0.100000\n",
      "\n",
      "Epoch [3][7/12]\t Loss 0.6570 0.6460\tTop1 Acc 65.039 66.323\tLR 0.100000\n",
      "\n",
      "Epoch [3][8/12]\t Loss 0.6505 0.6466\tTop1 Acc 65.625 66.235\tLR 0.100000\n",
      "\n",
      "Epoch [3][9/12]\t Loss 0.6283 0.6445\tTop1 Acc 67.969 66.428\tLR 0.100000\n",
      "\n",
      "Epoch [3][10/12]\t Loss 0.6192 0.6420\tTop1 Acc 68.945 66.680\tLR 0.100000\n",
      "\n",
      "Epoch [3][11/12]\t Loss 0.6357 0.6414\tTop1 Acc 67.188 66.726\tLR 0.100000\n",
      "\n",
      "Epoch [3][12/12]\t Loss 0.6364 0.6410\tTop1 Acc 67.188 66.764\tLR 0.100000\n",
      "\n",
      "Epoch [4][1/12]\t Loss 0.6568 0.6568\tTop1 Acc 64.648 64.648\tLR 0.100000\n",
      "\n",
      "Epoch [4][2/12]\t Loss 0.6336 0.6452\tTop1 Acc 67.188 65.918\tLR 0.100000\n",
      "\n",
      "Epoch [4][3/12]\t Loss 0.6765 0.6557\tTop1 Acc 62.109 64.648\tLR 0.100000\n",
      "\n",
      "Epoch [4][4/12]\t Loss 0.6335 0.6501\tTop1 Acc 67.188 65.283\tLR 0.100000\n",
      "\n",
      "Epoch [4][5/12]\t Loss 0.6236 0.6448\tTop1 Acc 68.359 65.898\tLR 0.100000\n",
      "\n",
      "Epoch [4][6/12]\t Loss 0.6443 0.6447\tTop1 Acc 65.820 65.885\tLR 0.100000\n",
      "\n",
      "Epoch [4][7/12]\t Loss 0.6281 0.6424\tTop1 Acc 67.773 66.155\tLR 0.100000\n",
      "\n",
      "Epoch [4][8/12]\t Loss 0.6240 0.6401\tTop1 Acc 68.164 66.406\tLR 0.100000\n",
      "\n",
      "Epoch [4][9/12]\t Loss 0.6290 0.6388\tTop1 Acc 67.578 66.536\tLR 0.100000\n",
      "\n",
      "Epoch [4][10/12]\t Loss 0.6289 0.6379\tTop1 Acc 67.578 66.641\tLR 0.100000\n",
      "\n",
      "Epoch [4][11/12]\t Loss 0.6314 0.6373\tTop1 Acc 67.188 66.690\tLR 0.100000\n",
      "\n",
      "Epoch [4][12/12]\t Loss 0.6217 0.6360\tTop1 Acc 68.555 66.846\tLR 0.100000\n",
      "\n",
      "Epoch [5][1/12]\t Loss 0.6041 0.6041\tTop1 Acc 70.703 70.703\tLR 0.100000\n",
      "\n",
      "Epoch [5][2/12]\t Loss 0.6142 0.6092\tTop1 Acc 69.336 70.020\tLR 0.100000\n",
      "\n",
      "Epoch [5][3/12]\t Loss 0.6349 0.6178\tTop1 Acc 66.602 68.880\tLR 0.100000\n",
      "\n",
      "Epoch [5][4/12]\t Loss 0.6017 0.6138\tTop1 Acc 71.094 69.434\tLR 0.100000\n",
      "\n",
      "Epoch [5][5/12]\t Loss 0.6259 0.6162\tTop1 Acc 67.773 69.102\tLR 0.100000\n",
      "\n",
      "Epoch [5][6/12]\t Loss 0.6179 0.6165\tTop1 Acc 68.945 69.076\tLR 0.100000\n",
      "\n",
      "Epoch [5][7/12]\t Loss 0.6127 0.6159\tTop1 Acc 69.531 69.141\tLR 0.100000\n",
      "\n",
      "Epoch [5][8/12]\t Loss 0.6283 0.6175\tTop1 Acc 67.578 68.945\tLR 0.100000\n",
      "\n",
      "Epoch [5][9/12]\t Loss 0.6439 0.6204\tTop1 Acc 65.234 68.533\tLR 0.100000\n",
      "\n",
      "Epoch [5][10/12]\t Loss 0.6323 0.6216\tTop1 Acc 66.797 68.359\tLR 0.100000\n",
      "\n",
      "Epoch [5][11/12]\t Loss 0.6002 0.6197\tTop1 Acc 71.289 68.626\tLR 0.100000\n",
      "\n",
      "Epoch [5][12/12]\t Loss 0.6287 0.6204\tTop1 Acc 67.383 68.522\tLR 0.100000\n",
      "\n",
      "Epoch [6][1/12]\t Loss 0.6235 0.6235\tTop1 Acc 68.164 68.164\tLR 0.100000\n",
      "\n",
      "Epoch [6][2/12]\t Loss 0.6232 0.6233\tTop1 Acc 67.969 68.066\tLR 0.100000\n",
      "\n",
      "Epoch [6][3/12]\t Loss 0.6161 0.6209\tTop1 Acc 68.945 68.359\tLR 0.100000\n",
      "\n",
      "Epoch [6][4/12]\t Loss 0.6268 0.6224\tTop1 Acc 67.578 68.164\tLR 0.100000\n",
      "\n",
      "Epoch [6][5/12]\t Loss 0.5999 0.6179\tTop1 Acc 70.703 68.672\tLR 0.100000\n",
      "\n",
      "Epoch [6][6/12]\t Loss 0.6196 0.6182\tTop1 Acc 68.359 68.620\tLR 0.100000\n",
      "\n",
      "Epoch [6][7/12]\t Loss 0.6297 0.6198\tTop1 Acc 66.602 68.331\tLR 0.100000\n",
      "\n",
      "Epoch [6][8/12]\t Loss 0.6103 0.6186\tTop1 Acc 69.336 68.457\tLR 0.100000\n",
      "\n",
      "Epoch [6][9/12]\t Loss 0.6230 0.6191\tTop1 Acc 67.773 68.381\tLR 0.100000\n",
      "\n",
      "Epoch [6][10/12]\t Loss 0.6067 0.6179\tTop1 Acc 69.727 68.516\tLR 0.100000\n",
      "\n",
      "Epoch [6][11/12]\t Loss 0.6184 0.6179\tTop1 Acc 67.969 68.466\tLR 0.100000\n",
      "\n",
      "Epoch [6][12/12]\t Loss 0.6120 0.6174\tTop1 Acc 68.945 68.506\tLR 0.100000\n",
      "\n",
      "Epoch [7][1/12]\t Loss 0.6124 0.6124\tTop1 Acc 68.750 68.750\tLR 0.100000\n",
      "\n",
      "Epoch [7][2/12]\t Loss 0.6019 0.6071\tTop1 Acc 70.312 69.531\tLR 0.100000\n",
      "\n",
      "Epoch [7][3/12]\t Loss 0.6247 0.6130\tTop1 Acc 66.992 68.685\tLR 0.100000\n",
      "\n",
      "Epoch [7][4/12]\t Loss 0.5869 0.6065\tTop1 Acc 72.070 69.531\tLR 0.100000\n",
      "\n",
      "Epoch [7][5/12]\t Loss 0.6164 0.6085\tTop1 Acc 67.969 69.219\tLR 0.100000\n",
      "\n",
      "Epoch [7][6/12]\t Loss 0.6443 0.6145\tTop1 Acc 64.258 68.392\tLR 0.100000\n",
      "\n",
      "Epoch [7][7/12]\t Loss 0.6406 0.6182\tTop1 Acc 64.453 67.829\tLR 0.100000\n",
      "\n",
      "Epoch [7][8/12]\t Loss 0.6311 0.6198\tTop1 Acc 66.016 67.603\tLR 0.100000\n",
      "\n",
      "Epoch [7][9/12]\t Loss 0.6044 0.6181\tTop1 Acc 69.531 67.817\tLR 0.100000\n",
      "\n",
      "Epoch [7][10/12]\t Loss 0.6198 0.6183\tTop1 Acc 66.992 67.734\tLR 0.100000\n",
      "\n",
      "Epoch [7][11/12]\t Loss 0.6048 0.6170\tTop1 Acc 68.945 67.844\tLR 0.100000\n",
      "\n",
      "Epoch [7][12/12]\t Loss 0.6314 0.6182\tTop1 Acc 65.430 67.643\tLR 0.100000\n",
      "\n",
      "Epoch [8][1/12]\t Loss 0.6185 0.6185\tTop1 Acc 66.797 66.797\tLR 0.100000\n",
      "\n",
      "Epoch [8][2/12]\t Loss 0.5933 0.6059\tTop1 Acc 70.508 68.652\tLR 0.100000\n",
      "\n",
      "Epoch [8][3/12]\t Loss 0.5781 0.5966\tTop1 Acc 72.266 69.857\tLR 0.100000\n",
      "\n",
      "Epoch [8][4/12]\t Loss 0.5874 0.5943\tTop1 Acc 70.703 70.068\tLR 0.100000\n",
      "\n",
      "Epoch [8][5/12]\t Loss 0.6148 0.5984\tTop1 Acc 67.578 69.570\tLR 0.100000\n",
      "\n",
      "Epoch [8][6/12]\t Loss 0.6282 0.6034\tTop1 Acc 64.648 68.750\tLR 0.100000\n",
      "\n",
      "Epoch [8][7/12]\t Loss 0.6138 0.6049\tTop1 Acc 66.797 68.471\tLR 0.100000\n",
      "\n",
      "Epoch [8][8/12]\t Loss 0.6292 0.6079\tTop1 Acc 64.062 67.920\tLR 0.100000\n",
      "\n",
      "Epoch [8][9/12]\t Loss 0.6001 0.6070\tTop1 Acc 68.359 67.969\tLR 0.100000\n",
      "\n",
      "Epoch [8][10/12]\t Loss 0.6124 0.6076\tTop1 Acc 66.602 67.832\tLR 0.100000\n",
      "\n",
      "Epoch [8][11/12]\t Loss 0.5807 0.6051\tTop1 Acc 70.898 68.111\tLR 0.100000\n",
      "\n",
      "Epoch [8][12/12]\t Loss 0.5742 0.6025\tTop1 Acc 70.898 68.343\tLR 0.100000\n",
      "\n",
      "Epoch [9][1/12]\t Loss 0.5965 0.5965\tTop1 Acc 67.578 67.578\tLR 0.100000\n",
      "\n",
      "Epoch [9][2/12]\t Loss 0.5951 0.5958\tTop1 Acc 68.750 68.164\tLR 0.100000\n",
      "\n",
      "Epoch [9][3/12]\t Loss 0.6035 0.5983\tTop1 Acc 66.016 67.448\tLR 0.100000\n",
      "\n",
      "Epoch [9][4/12]\t Loss 0.5813 0.5941\tTop1 Acc 69.141 67.871\tLR 0.100000\n",
      "\n",
      "Epoch [9][5/12]\t Loss 0.6011 0.5955\tTop1 Acc 66.016 67.500\tLR 0.100000\n",
      "\n",
      "Epoch [9][6/12]\t Loss 0.5986 0.5960\tTop1 Acc 66.406 67.318\tLR 0.100000\n",
      "\n",
      "Epoch [9][7/12]\t Loss 0.5729 0.5927\tTop1 Acc 69.922 67.690\tLR 0.100000\n",
      "\n",
      "Epoch [9][8/12]\t Loss 0.5877 0.5921\tTop1 Acc 67.969 67.725\tLR 0.100000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9][9/12]\t Loss 0.5637 0.5889\tTop1 Acc 69.922 67.969\tLR 0.100000\n",
      "\n",
      "Epoch [9][10/12]\t Loss 0.5911 0.5891\tTop1 Acc 64.648 67.637\tLR 0.100000\n",
      "\n",
      "Epoch [9][11/12]\t Loss 0.5534 0.5859\tTop1 Acc 71.484 67.987\tLR 0.100000\n",
      "\n",
      "Epoch [9][12/12]\t Loss 0.5852 0.5858\tTop1 Acc 66.016 67.822\tLR 0.100000\n",
      "\n",
      "Epoch [10][1/12]\t Loss 0.5457 0.5457\tTop1 Acc 69.922 69.922\tLR 0.100000\n",
      "\n",
      "Epoch [10][2/12]\t Loss 0.5492 0.5474\tTop1 Acc 69.336 69.629\tLR 0.100000\n",
      "\n",
      "Epoch [10][3/12]\t Loss 0.5484 0.5478\tTop1 Acc 69.922 69.727\tLR 0.100000\n",
      "\n",
      "Epoch [10][4/12]\t Loss 0.5526 0.5490\tTop1 Acc 71.094 70.068\tLR 0.100000\n",
      "\n",
      "Epoch [10][5/12]\t Loss 0.5443 0.5480\tTop1 Acc 71.289 70.312\tLR 0.100000\n",
      "\n",
      "Epoch [10][6/12]\t Loss 0.5333 0.5456\tTop1 Acc 74.414 70.996\tLR 0.100000\n",
      "\n",
      "Epoch [10][7/12]\t Loss 0.5179 0.5416\tTop1 Acc 78.906 72.126\tLR 0.100000\n",
      "\n",
      "Epoch [10][8/12]\t Loss 0.5313 0.5403\tTop1 Acc 78.711 72.949\tLR 0.100000\n",
      "\n",
      "Epoch [10][9/12]\t Loss 0.5062 0.5365\tTop1 Acc 83.594 74.132\tLR 0.100000\n",
      "\n",
      "Epoch [10][10/12]\t Loss 0.5126 0.5341\tTop1 Acc 83.008 75.020\tLR 0.100000\n",
      "\n",
      "Epoch [10][11/12]\t Loss 0.5110 0.5320\tTop1 Acc 84.766 75.906\tLR 0.100000\n",
      "\n",
      "Epoch [10][12/12]\t Loss 0.5072 0.5300\tTop1 Acc 86.133 76.758\tLR 0.100000\n",
      "\n",
      "Epoch [11][1/12]\t Loss 0.4927 0.4927\tTop1 Acc 87.891 87.891\tLR 0.100000\n",
      "\n",
      "Epoch [11][2/12]\t Loss 0.4666 0.4797\tTop1 Acc 93.555 90.723\tLR 0.100000\n",
      "\n",
      "Epoch [11][3/12]\t Loss 0.4750 0.4781\tTop1 Acc 88.086 89.844\tLR 0.100000\n",
      "\n",
      "Epoch [11][4/12]\t Loss 0.4697 0.4760\tTop1 Acc 88.867 89.600\tLR 0.100000\n",
      "\n",
      "Epoch [11][5/12]\t Loss 0.4477 0.4703\tTop1 Acc 92.188 90.117\tLR 0.100000\n",
      "\n",
      "Epoch [11][6/12]\t Loss 0.4455 0.4662\tTop1 Acc 93.359 90.658\tLR 0.100000\n",
      "\n",
      "Epoch [11][7/12]\t Loss 0.4426 0.4628\tTop1 Acc 93.945 91.127\tLR 0.100000\n",
      "\n",
      "Epoch [11][8/12]\t Loss 0.4280 0.4585\tTop1 Acc 94.922 91.602\tLR 0.100000\n",
      "\n",
      "Epoch [11][9/12]\t Loss 0.4210 0.4543\tTop1 Acc 96.289 92.122\tLR 0.100000\n",
      "\n",
      "Epoch [11][10/12]\t Loss 0.4068 0.4496\tTop1 Acc 96.875 92.598\tLR 0.100000\n",
      "\n",
      "Epoch [11][11/12]\t Loss 0.4064 0.4456\tTop1 Acc 95.898 92.898\tLR 0.100000\n",
      "\n",
      "Epoch [11][12/12]\t Loss 0.3878 0.4408\tTop1 Acc 98.242 93.343\tLR 0.100000\n",
      "\n",
      "Epoch [12][1/12]\t Loss 0.3833 0.3833\tTop1 Acc 96.484 96.484\tLR 0.100000\n",
      "\n",
      "Epoch [12][2/12]\t Loss 0.3800 0.3817\tTop1 Acc 97.852 97.168\tLR 0.100000\n",
      "\n",
      "Epoch [12][3/12]\t Loss 0.3745 0.3793\tTop1 Acc 97.852 97.396\tLR 0.100000\n",
      "\n",
      "Epoch [12][4/12]\t Loss 0.3655 0.3758\tTop1 Acc 98.438 97.656\tLR 0.100000\n",
      "\n",
      "Epoch [12][5/12]\t Loss 0.3592 0.3725\tTop1 Acc 99.023 97.930\tLR 0.100000\n",
      "\n",
      "Epoch [12][6/12]\t Loss 0.3569 0.3699\tTop1 Acc 98.047 97.949\tLR 0.100000\n",
      "\n",
      "Epoch [12][7/12]\t Loss 0.3511 0.3672\tTop1 Acc 99.023 98.103\tLR 0.100000\n",
      "\n",
      "Epoch [12][8/12]\t Loss 0.3472 0.3647\tTop1 Acc 99.219 98.242\tLR 0.100000\n",
      "\n",
      "Epoch [12][9/12]\t Loss 0.3465 0.3627\tTop1 Acc 99.609 98.394\tLR 0.100000\n",
      "\n",
      "Epoch [12][10/12]\t Loss 0.3479 0.3612\tTop1 Acc 98.633 98.418\tLR 0.100000\n",
      "\n",
      "Epoch [12][11/12]\t Loss 0.3400 0.3593\tTop1 Acc 99.023 98.473\tLR 0.100000\n",
      "\n",
      "Epoch [12][12/12]\t Loss 0.3370 0.3574\tTop1 Acc 98.828 98.503\tLR 0.100000\n",
      "\n",
      "Epoch [13][1/12]\t Loss 0.3317 0.3317\tTop1 Acc 99.609 99.609\tLR 0.100000\n",
      "\n",
      "Epoch [13][2/12]\t Loss 0.3400 0.3358\tTop1 Acc 98.633 99.121\tLR 0.100000\n",
      "\n",
      "Epoch [13][3/12]\t Loss 0.3300 0.3339\tTop1 Acc 99.414 99.219\tLR 0.100000\n",
      "\n",
      "Epoch [13][4/12]\t Loss 0.3351 0.3342\tTop1 Acc 98.242 98.975\tLR 0.100000\n",
      "\n",
      "Epoch [13][5/12]\t Loss 0.3350 0.3344\tTop1 Acc 98.242 98.828\tLR 0.100000\n",
      "\n",
      "Epoch [13][6/12]\t Loss 0.3339 0.3343\tTop1 Acc 98.828 98.828\tLR 0.100000\n",
      "\n",
      "Epoch [13][7/12]\t Loss 0.3255 0.3330\tTop1 Acc 99.219 98.884\tLR 0.100000\n",
      "\n",
      "Epoch [13][8/12]\t Loss 0.3341 0.3332\tTop1 Acc 98.438 98.828\tLR 0.100000\n",
      "\n",
      "Epoch [13][9/12]\t Loss 0.3271 0.3325\tTop1 Acc 98.828 98.828\tLR 0.100000\n",
      "\n",
      "Epoch [13][10/12]\t Loss 0.3250 0.3318\tTop1 Acc 99.414 98.887\tLR 0.100000\n",
      "\n",
      "Epoch [13][11/12]\t Loss 0.3279 0.3314\tTop1 Acc 99.219 98.917\tLR 0.100000\n",
      "\n",
      "Epoch [13][12/12]\t Loss 0.3297 0.3313\tTop1 Acc 98.828 98.910\tLR 0.100000\n",
      "\n",
      "Epoch [14][1/12]\t Loss 0.3285 0.3285\tTop1 Acc 99.023 99.023\tLR 0.100000\n",
      "\n",
      "Epoch [14][2/12]\t Loss 0.3280 0.3283\tTop1 Acc 98.828 98.926\tLR 0.100000\n",
      "\n",
      "Epoch [14][3/12]\t Loss 0.3251 0.3272\tTop1 Acc 99.219 99.023\tLR 0.100000\n",
      "\n",
      "Epoch [14][4/12]\t Loss 0.3313 0.3282\tTop1 Acc 98.633 98.926\tLR 0.100000\n",
      "\n",
      "Epoch [14][5/12]\t Loss 0.3256 0.3277\tTop1 Acc 99.219 98.984\tLR 0.100000\n",
      "\n",
      "Epoch [14][6/12]\t Loss 0.3251 0.3273\tTop1 Acc 99.219 99.023\tLR 0.100000\n",
      "\n",
      "Epoch [14][7/12]\t Loss 0.3243 0.3268\tTop1 Acc 99.023 99.023\tLR 0.100000\n",
      "\n",
      "Epoch [14][8/12]\t Loss 0.3287 0.3271\tTop1 Acc 98.633 98.975\tLR 0.100000\n",
      "\n",
      "Epoch [14][9/12]\t Loss 0.3266 0.3270\tTop1 Acc 99.023 98.980\tLR 0.100000\n",
      "\n",
      "Epoch [14][10/12]\t Loss 0.3236 0.3267\tTop1 Acc 99.219 99.004\tLR 0.100000\n",
      "\n",
      "Epoch [14][11/12]\t Loss 0.3269 0.3267\tTop1 Acc 99.023 99.006\tLR 0.100000\n",
      "\n",
      "Epoch [14][12/12]\t Loss 0.3222 0.3263\tTop1 Acc 99.414 99.040\tLR 0.100000\n",
      "\n",
      "Epoch [15][1/12]\t Loss 0.3282 0.3282\tTop1 Acc 98.828 98.828\tLR 0.100000\n",
      "\n",
      "Epoch [15][2/12]\t Loss 0.3199 0.3241\tTop1 Acc 99.805 99.316\tLR 0.100000\n",
      "\n",
      "Epoch [15][3/12]\t Loss 0.3223 0.3235\tTop1 Acc 99.609 99.414\tLR 0.100000\n",
      "\n",
      "Epoch [15][4/12]\t Loss 0.3221 0.3231\tTop1 Acc 99.414 99.414\tLR 0.100000\n",
      "\n",
      "Epoch [15][5/12]\t Loss 0.3221 0.3229\tTop1 Acc 99.219 99.375\tLR 0.100000\n",
      "\n",
      "Epoch [15][6/12]\t Loss 0.3301 0.3241\tTop1 Acc 98.438 99.219\tLR 0.100000\n",
      "\n",
      "Epoch [15][7/12]\t Loss 0.3205 0.3236\tTop1 Acc 99.805 99.302\tLR 0.100000\n",
      "\n",
      "Epoch [15][8/12]\t Loss 0.3203 0.3232\tTop1 Acc 99.609 99.341\tLR 0.100000\n",
      "\n",
      "Epoch [15][9/12]\t Loss 0.3303 0.3240\tTop1 Acc 98.438 99.240\tLR 0.100000\n",
      "\n",
      "Epoch [15][10/12]\t Loss 0.3294 0.3245\tTop1 Acc 98.633 99.180\tLR 0.100000\n",
      "\n",
      "Epoch [15][11/12]\t Loss 0.3235 0.3244\tTop1 Acc 99.219 99.183\tLR 0.100000\n",
      "\n",
      "Epoch [15][12/12]\t Loss 0.3284 0.3248\tTop1 Acc 98.438 99.121\tLR 0.100000\n",
      "\n",
      "Epoch [16][1/12]\t Loss 0.3193 0.3193\tTop1 Acc 99.414 99.414\tLR 0.100000\n",
      "\n",
      "Epoch [16][2/12]\t Loss 0.3273 0.3233\tTop1 Acc 98.633 99.023\tLR 0.100000\n",
      "\n",
      "Epoch [16][3/12]\t Loss 0.3290 0.3252\tTop1 Acc 98.633 98.893\tLR 0.100000\n",
      "\n",
      "Epoch [16][4/12]\t Loss 0.3242 0.3249\tTop1 Acc 99.023 98.926\tLR 0.100000\n",
      "\n",
      "Epoch [16][5/12]\t Loss 0.3280 0.3255\tTop1 Acc 98.633 98.867\tLR 0.100000\n",
      "\n",
      "Epoch [16][6/12]\t Loss 0.3193 0.3245\tTop1 Acc 99.805 99.023\tLR 0.100000\n",
      "\n",
      "Epoch [16][7/12]\t Loss 0.3216 0.3241\tTop1 Acc 99.219 99.051\tLR 0.100000\n",
      "\n",
      "Epoch [16][8/12]\t Loss 0.3249 0.3242\tTop1 Acc 99.219 99.072\tLR 0.100000\n",
      "\n",
      "Epoch [16][9/12]\t Loss 0.3232 0.3241\tTop1 Acc 99.023 99.067\tLR 0.100000\n",
      "\n",
      "Epoch [16][10/12]\t Loss 0.3219 0.3239\tTop1 Acc 99.219 99.082\tLR 0.100000\n",
      "\n",
      "Epoch [16][11/12]\t Loss 0.3179 0.3233\tTop1 Acc 99.805 99.148\tLR 0.100000\n",
      "\n",
      "Epoch [16][12/12]\t Loss 0.3218 0.3232\tTop1 Acc 99.023 99.137\tLR 0.100000\n",
      "\n",
      "Epoch [17][1/12]\t Loss 0.3243 0.3243\tTop1 Acc 98.828 98.828\tLR 0.100000\n",
      "\n",
      "Epoch [17][2/12]\t Loss 0.3246 0.3245\tTop1 Acc 99.219 99.023\tLR 0.100000\n",
      "\n",
      "Epoch [17][3/12]\t Loss 0.3283 0.3257\tTop1 Acc 98.438 98.828\tLR 0.100000\n",
      "\n",
      "Epoch [17][4/12]\t Loss 0.3199 0.3243\tTop1 Acc 99.609 99.023\tLR 0.100000\n",
      "\n",
      "Epoch [17][5/12]\t Loss 0.3187 0.3231\tTop1 Acc 99.805 99.180\tLR 0.100000\n",
      "\n",
      "Epoch [17][6/12]\t Loss 0.3210 0.3228\tTop1 Acc 99.219 99.186\tLR 0.100000\n",
      "\n",
      "Epoch [17][7/12]\t Loss 0.3230 0.3228\tTop1 Acc 99.219 99.191\tLR 0.100000\n",
      "\n",
      "Epoch [17][8/12]\t Loss 0.3251 0.3231\tTop1 Acc 99.023 99.170\tLR 0.100000\n",
      "\n",
      "Epoch [17][9/12]\t Loss 0.3233 0.3231\tTop1 Acc 99.023 99.154\tLR 0.100000\n",
      "\n",
      "Epoch [17][10/12]\t Loss 0.3222 0.3230\tTop1 Acc 99.023 99.141\tLR 0.100000\n",
      "\n",
      "Epoch [17][11/12]\t Loss 0.3239 0.3231\tTop1 Acc 98.828 99.112\tLR 0.100000\n",
      "\n",
      "Epoch [17][12/12]\t Loss 0.3164 0.3226\tTop1 Acc 99.805 99.170\tLR 0.100000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m80\u001b[39m):\n\u001b[1;32m     26\u001b[0m     losses, top1 \u001b[38;5;241m=\u001b[39m AverageMeter(), AverageMeter()\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,(data,label,label_v) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     29\u001b[0m         data,label,label_v \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:3\u001b[39m\u001b[38;5;124m'\u001b[39m), label\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:3\u001b[39m\u001b[38;5;124m'\u001b[39m), label_v\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m         outputs,values \u001b[38;5;241m=\u001b[39m model_deter(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/ISSV/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/ISSV/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ISSV/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/ISSV/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ISSV/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/ISSV/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import sys \n",
    "from utils.utils import AverageMeter,get_lr\n",
    "\n",
    "batch_size=512\n",
    "dataset = WavDataset(spk_list, spk2utt=spk2utt, embd_dict=embds_dicts)\n",
    "batch_sampler = WavBatchSampler(dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "train_loader = DataLoader(dataset, batch_sampler=batch_sampler, num_workers=12, pin_memory=True)\n",
    "\n",
    "    \n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "# model_deter= Binary_deter_net(256,256).to('cuda:3')\n",
    "model_deter= Multi_deter_net(256,256).to('cuda:3')\n",
    "\n",
    "optimizer = torch.optim.SGD(list(model_deter.parameters()),\n",
    "                            lr=0.1, momentum=0.95, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [20,40,60], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "model_deter.train()\n",
    "\n",
    "\n",
    "for epoch in range(0,80):\n",
    "    losses, top1 = AverageMeter(), AverageMeter()\n",
    "    for i,(data,label,label_v) in enumerate(train_loader):\n",
    "    \n",
    "        data,label,label_v = data.to('cuda:3'), label.long().to('cuda:3'), label_v.float().to('cuda:3')\n",
    "\n",
    "        outputs,values = model_deter(data)\n",
    "        loss1 = criterion_ce(outputs,label) # cross entropy loss\n",
    "        loss2 = criterion_mse(values,label_v) # mse loss\n",
    "        loss= loss1+loss2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        prec1 = acc_topk(outputs.data, label)*100\n",
    "        losses.update(loss.data.item(), batch_size)\n",
    "        top1.update(prec1, batch_size)\n",
    "\n",
    "        print('Epoch [%d][%d/%d]\\t ' % (epoch, i+1, len(train_loader)) + \n",
    "              'Loss %.4f %.4f\\t' % (losses.val, losses.avg) +\n",
    "              'Top1 Acc %3.3f %3.3f\\t' % (top1.val, top1.avg) +\n",
    "              'LR %.6f\\n' % get_lr(optimizer))\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12eaa665",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': model_deter.state_dict()},\n",
    "           './DRL-TU/egs/exp/DRL-TU-MH_pretrained/pretrain_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3d4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISSV",
   "language": "python",
   "name": "issv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
